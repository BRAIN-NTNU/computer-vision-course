{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digit Recognition\n",
    "### Using MNIST, Keras and OpenCV\n",
    "---\n",
    "\n",
    "**Dataset**: [MNIST](http://yann.lecun.com/exdb/mnist/)\n",
    "* `x_train, y_train`: contains  60 000 training images, and labels\n",
    "* `x_test, y_test`: contains  10 000 test images, and labels\n",
    "\n",
    "\n",
    "**Idea**: We will train a Fully Connected Neural Network to perform real time prediction on hand written images using your built in PC-webcamera.\n",
    "\n",
    "> A key part in achieving this is succesfully preprocessing the images fed  from our webcamera to be as equal to the images the model will be trained on. In other words, we will attempt to create a MNIST preprocessing pipeline.\n",
    "\n",
    "By [William Kvaale](https://github.com/wQuole)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the libraries and modules needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# NumPy - math library for handling matrices and arrays\n",
    "import numpy as np\n",
    "\n",
    "# MNIST dataset containing over 60 000 training and 10 000 test examples\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# OpenCV - Opensource Computer Vision library\n",
    "import cv2 as cv\n",
    "\n",
    "# Matplotlib's PyPlot - visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pandas - Data Analysis library\n",
    "import pandas as pd\n",
    "\n",
    "#  Keras is Tensorflow's high level API for Deep Learning\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and visualize the data\n",
    "<details><summary>Hint: </summary>\n",
    "<p>\n",
    "    \n",
    "https://keras.io/api/datasets/mnist/\n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = # TODO ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_distribution(data):\n",
    "    df = pd.DataFrame.from_records([data])\n",
    "    df = df.T\n",
    "    df.rename(columns = {0: 'target'}, inplace=True)\n",
    "    count = df['target'].value_counts().sort_index()\n",
    "    count.plot.bar(color=plt.cm.Paired(np.arange(len(count))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_distribution(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_distribution(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss with your partner**:\n",
    "\n",
    "what _could_ be the result of different distribution in training- and test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize an example of every digit (0-9) from the training set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    ith_number = np.where(y_train == i)[0]\n",
    "    random_index_number = ith_number[np.random.randint(0, (len(ith_number)))]\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.title(f\"{y_train[random_index_number]}\", y=-0.4, fontdict={'fontsize':14, 'color': 'green'})\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x_train[random_index_number], cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Any thoughts on the data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "**What does our data look like, raw?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "print(f\"y_train:\\n{y_train[0]}\")\n",
    "\n",
    "print(f\"x_train:\\n{x_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> Click here for a more clear example  </summary>\n",
    "<p>\n",
    "    \n",
    "<img src=\"images/mnist_array_example.jpeg\" width=\"500\"/>\n",
    "<a href=\"https://towardsdatascience.com/how-to-teach-a-computer-to-see-with-convolutional-neural-networks-96c120827cd1\">[Image Source]</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the shape (dimensions) of our data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We would like to add another dimension to our images\n",
    "In other words, converting the shape from (28, 28) to (28, 28, 1).\n",
    "\n",
    "This will describe that our image has _one_ channel, and is grayscale. \n",
    "> FYI: A colored (RGB) image has 3 channels (e.g. (28, 28, 3))\n",
    "\n",
    "We do this in order to be able to use **Keras' Conv2D layer** which expects the format of the data to be: `(batch_size, height, width, channels)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Hint</summary>\n",
    "<p>\n",
    "        \n",
    "[Link to Numpy docs (np.reshape)](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html)\n",
    "    \n",
    "```python\n",
    "data.reshape(NUMBER_OF_IMAGES, HEIGHT, WIDTH, CHANNELS)\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = # TODO ...\n",
    "x_test = # TODO ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data\n",
    "The pixel intensity in a images is in the range between `[0, 255]`. We would like to normalize this to `[0.0, 1.0]`.\n",
    "> Artificial Neural Network processes the input data by updating its so called weights. This process can be disrupted by large integer values, and the training process can be slowed down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    data = data.astype('float32')\n",
    "    data /= # TODO ...\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train)\n",
    "x_test = normalize(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "Most machine learning algorithms are not able to train on labeled data, e.g. \"cat\", \"dog\". This needs to be converted into a numerical representation. This step is called **integer encoding**. However, by just converting \"cat\" to 1, and \"dog\" to 2, we are making an unjustified assumption that there exists a **natural ordinal ordering**. This would be the case if our data were, e.g.,  $\\{male,\\ female,\\ child\\} \\rightarrow \\{0, 1 , 2\\}$ as in the Titanic data set used in the [Intro to Data Science Course](https://github.com/BRAIN-NTNU/data-science-course/blob/master/%5BSolution%5D%20Data%20Science%20Course%20-%20Titanic.ipynb).\n",
    "\n",
    "Therefore, we need to **one-hot encode** our data. This way we can represent categorical data, in a numerical way, without assuming anything about its ordering.\n",
    "![image-2.png](images/one_hot_encoding.jpeg)\n",
    "[[Image source](https://medium.com/@michaeldelsole/what-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Hint: </summary>\n",
    "<p>\n",
    "    \n",
    "https://keras.io/api/utils/python_utils/#to_categorical-function\n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Hint 2: </summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "    data = to_categorical(data, num_classes=NUM_CLASSES)\n",
    "```\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = # TODO ...\n",
    "y_test = # TODO ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set a `random_number` to visualize a example from the dataset**\n",
    "Can you find a example where you would wrongly, a human, would fail at labeling the image correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Hint: </summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "    random_number = np.random.randint(0, len(x_train))\n",
    "```\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_number = # TODO ...\n",
    "\n",
    "number = np.where(y_train[random_number] == 1)[0][0]\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(x_train[random_number], cmap='gray')\n",
    "plt.title(f\"y: {number}\", fontdict={'fontsize':16, 'color': 'green'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Let's define a model\n",
    "\n",
    "We will define a Vanilla Fully Connected Feed Forward Neural Network. This might sound very complicated, but in should not discourage the upcomming machine learning apprentice.\n",
    "\n",
    "In order for the network to find non-linear relationships in the data, we use **activation functions**. One activation function that is quite common these day are the ReLu function.\n",
    "$$ ReLu = max(0, x)$$\n",
    "\n",
    "\n",
    "Our network will output a `1x10 vector` representing the _probability_ for each number. This is done using **softmax**. \n",
    "$$ S(y_i) = \\frac{e^{y_i}}{\\sum_{i=0}^{n=9}{e^{y_i}}}$$\n",
    "\n",
    "All the probabilities inside the outputted vector will sum to 1.\n",
    "You can read more about Softmax [here](https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d).\n",
    "\n",
    "### Our model\n",
    "Is composed using the [Sequential class from Keras](https://keras.io/api/models/sequential/), and we will add three layers:\n",
    "1. a [Flatten](https://keras.io/api/layers/reshaping_layers/flatten/) layer with input_shape=(28, 28, 1)\n",
    "2. a [Dense](https://keras.io/api/layers/core_layers/dense/) layer with 128 units and **relu** as our activation function\n",
    "3. another Dense layer, with 10 units and **softmax** activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1: </summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "model.add(Flatten(input_shape=(28, 28, 1)))\n",
    "```\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Hint 2: </summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "model.add(Dense(128, activation='relu'))\n",
    "```\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Hint 3: </summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "```\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "\n",
    "model.add(# TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    Architecture Visualized\n",
    "</h3>\n",
    "<img src=\"fcnn.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Node-Link Visualization of:\n",
    "#### [Fully Connected Neural Networks](https://www.cs.ryerson.ca/~aharley/vis/fc/)\n",
    "\n",
    "#### [Convolutional Neural Networks](https://www.cs.ryerson.ca/~aharley/vis/conv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "In order to use our defined model, we need to _compile_ it. This will create a [computational graph](https://medium.com/tebs-lab/deep-neural-networks-as-computational-graphs-867fcaa56c9). We will need to choose a optimizer, and a loss function. In our case, `adam` is a good fit. You can read more about the [Adam Optimizer here](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c). As we have categorical data, we will choose `categorical_crossentropy`as our loss function. Read more on [loss functions here](https://towardsdatascience.com/what-is-loss-function-1e2605aeb904)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Hint: </summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "```\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=# TODO ...,\n",
    "    loss=# TODO ...,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Visualize accuracy and loss during training\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'loss'], loc='best')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('models/my_first_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('models/simple_fnn_30_epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate our model\n",
    "In order to evaluate our model, we evaluate it on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Hint: </summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "score = model.evaluate(X_TESTING_DATA, Y_TESTING_DATA, verbose=True)\n",
    "```\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy  = model.evaluate(x_test, y_test, verbose=True)\n",
    "\n",
    "print(f'Test loss: {loss:.3}')\n",
    "print(f'Test accuracy: {accuracy:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5. Let's have some fun\n",
    "\n",
    "Here we'll use OpenCV and our trained model to perform predictions on images fed from our webcamera, in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# MNIST PREPROCESSING\n",
    "# Inspired by Ole Kröger (https://bit.ly/3jPiBMY)\n",
    "import math\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "def get_best_shift(img):\n",
    "    center_y , center_x = ndimage.measurements.center_of_mass(img)\n",
    "\n",
    "    rows, cols = img.shape\n",
    "    shift_y = np.round(rows//2 - center_y)\n",
    "    shift_x = np.round(cols//2 - center_x)\n",
    "\n",
    "    return shift_x, shift_y\n",
    "\n",
    "\n",
    "def shift(img, sx, sy):\n",
    "    rows,cols = img.shape\n",
    "    M = np.float32([[1,0,sx],[0,1,sy]])\n",
    "    shifted = cv.warpAffine(img,M,(cols,rows))\n",
    "    return shifted\n",
    "\n",
    "\n",
    "def mnist_preprocessing(image, dim=28, box=20):\n",
    "    resized_image = cv.resize(image, dsize=(dim, dim), interpolation=cv.INTER_LINEAR)\n",
    "    gray = np.asarray(cv.cvtColor(resized_image, cv.COLOR_BGR2GRAY))\n",
    "    gray = 255 - gray\n",
    "    gray = cv.resize(gray, (dim, dim))\n",
    "    _, thresh = cv.threshold(gray, 128, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "    \n",
    "    \n",
    "    while np.sum(gray[0]) == 0:\n",
    "        gray = gray[1:]\n",
    "\n",
    "    while np.sum(gray[:,0]) == 0:\n",
    "        gray = np.delete(gray, 0, 1)\n",
    "\n",
    "    while np.sum(gray[-1]) == 0:\n",
    "        gray = gray[:-1]\n",
    "\n",
    "    while np.sum(gray[:,-1]) == 0:\n",
    "        gray = np.delete(gray, -1, 1)\n",
    "\n",
    "    rows,cols = gray.shape\n",
    "\n",
    "    if rows > cols:\n",
    "        factor = box/rows\n",
    "        rows = box\n",
    "        cols = int(round(cols*factor))\n",
    "        gray = cv.resize(gray, (cols,rows))\n",
    "    else:\n",
    "        factor = box/cols\n",
    "        cols = box\n",
    "        rows = int(round(rows*factor))\n",
    "        gray = cv.resize(gray, (cols, rows))\n",
    "\n",
    "    colsPadding = (int(math.ceil((dim - cols)/2.0)),int(math.floor((dim - cols)/2.0)))\n",
    "    rowsPadding = (int(math.ceil((dim - rows)/2.0)),int(math.floor((dim - rows)/2.0)))\n",
    "    gray = np.pad(gray, (rowsPadding, colsPadding), 'constant')\n",
    "\n",
    "    shiftx,shifty = get_best_shift(gray)\n",
    "    shifted = shift(gray,shiftx,shifty)\n",
    "    \n",
    "    normalized_img = normalize(shifted)\n",
    "    \n",
    "    return shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def predict_single_image(model, img):\n",
    "    img = np.expand_dims(img, axis=0)     # --> (1, 28, 28)\n",
    "    img = np.reshape(img, [1, 28, 28, 1]) # --> (1, 28, 28, 1)\n",
    "    pred = model.predict(img)\n",
    "    label = np.argmax(pred[:1])\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(0) \n",
    "\n",
    "init_frame_read_correctly, init_frame = cap.read()\n",
    "if not init_frame_read_correctly:\n",
    "        print(\"No frames to read...\")\n",
    "        \n",
    "\n",
    "bounding_box = (init_frame.shape[1]//4, init_frame.shape[0]//4,\n",
    "                init_frame.shape[1]//2, init_frame.shape[0]//2)    \n",
    "\n",
    "while(cap.isOpened()):\n",
    "    # Capture frame-by-frame\n",
    "    frame_read_correctly, frame = cap.read()\n",
    "    if not frame_read_correctly:\n",
    "        print(\"No frames to read...\")\n",
    "        break\n",
    "    \n",
    "    \n",
    "    # new_bounding_box = bounding_box\n",
    "    wh = (frame.shape[1]//frame.shape[0])*200\n",
    "    if frame_read_correctly:\n",
    "        point1 = (frame.shape[1]//2 - wh, frame.shape[0]//2 - wh)\n",
    "        point2 = (frame.shape[1]//2 + wh, frame.shape[0]//2 + wh)\n",
    "        cv.rectangle(frame, point1, point2, (255, 255, 0), 2)\n",
    "        \n",
    "\n",
    "    try:\n",
    "        # Crop out new ROI and process it\n",
    "        new_ROI = frame[frame.shape[0]//2 - wh :frame.shape[0]//2 + wh, frame.shape[1]//2 - wh:frame.shape[1]//2 + wh]\n",
    "        img = mnist_preprocessing(new_ROI)\n",
    "        label = predict_single_image(model, img)\n",
    "\n",
    "        cv.namedWindow('processed', 0)\n",
    "        cv.imshow('processed', img)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Exception occured:\\n{e}\")\n",
    "        pass\n",
    "\n",
    "    \n",
    "    # Display the resulting pred\n",
    "    cv.putText(frame,\n",
    "               f\"{label}\",\n",
    "               org=(point2[0] - 75, point2[1] + 75),\n",
    "               fontFace=cv.FONT_HERSHEY_PLAIN,\n",
    "               fontScale=5,\n",
    "               color=(255, 255, 0),\n",
    "               thickness=2,\n",
    "               lineType=cv.LINE_AA\n",
    "              )\n",
    "    \n",
    "    cv.namedWindow('frame', 0)\n",
    "    cv.resizeWindow('frame', 640, 480)\n",
    "    cv.imshow('frame', frame)\n",
    "    #cv.imshow('box', new_ROI)\n",
    "\n",
    "    # Listen for keypress\n",
    "    # SINCE OpenCV videoCapture inside a Jupyter Notebook Kernel is 'kinda broken'\n",
    "    # one needs to quit the OpenCV-windows hard..\n",
    "    # MacOS --> CMD+Q\n",
    "    # MacOS --> ALT+F4\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "    \n",
    "# Release the capture afterward\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('venv')",
   "language": "python",
   "name": "python38164bitvenv355e9b74feae425dba00ca460173104b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
