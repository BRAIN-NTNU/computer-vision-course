{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "In this tutorial you'll learn to use a pre trained model to classify objects in images. The input image can be of any size. Hopefully you will also get some understanding in how a model labels an image.\n",
    "\n",
    "This notebook is inspired by the following [blog post](https://www.learnopencv.com/fully-convolutional-image-classification-on-arbitrary-sized-image/).\n",
    "\n",
    "\n",
    "by Odd Eirik Igland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, hub\n",
    "from torchvision import models, transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of natural signals\n",
    "\n",
    "- *Locality*: things that are close are often equal\n",
    "- *Stationarity*: same pattern can be found several places in data\n",
    "- *Compositionality*: a complex expression is determined by its existing expressions and the combination between them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compositionality = cv2.imread(\"data/compositionality.png\")\n",
    "dog = cv2.imread(\"data/dog.jpeg\")\n",
    "rgb_image_1 = cv2.cvtColor(compositionality, cv2.COLOR_BGR2RGB)\n",
    "rgb_image_2 = cv2.cvtColor(dog, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "ax[0].imshow(rgb_image_1);\n",
    "ax[1].imshow(rgb_image_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image sources: [Marilyn Monroe](https://youtu.be/kwPWpVverkw?t=748), [dog](https://i.insider.com/5484d9d1eab8ea3017b17e29?width=1100&format=jpeg&auto=webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Node-Link Visualization of:\n",
    "\n",
    "### [Fully Connected Neural Networks](https://www.cs.ryerson.ca/~aharley/vis/fc/)\n",
    "\n",
    "### [Convolutional Neural Networks](https://www.cs.ryerson.ca/~aharley/vis/conv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet 18\n",
    "Image source: [resnet](https://i.imgur.com/XwcnU5x.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = cv2.imread(\"data/resnet18.png\")\n",
    "rgb_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize = (15,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(rgb_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and instanciate the model\n",
    "\n",
    "The model below is based on the ResNet-18 model. The expected input size for the network is 224Ã—224. We will customize it to use a convolutional layer instead of a fully-connected layer in the end to handle any image input size.\n",
    "\n",
    "By setting the pretrained variable to true the network weights will be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNet(models.ResNet):\n",
    "    def __init__(self, num_classes=1000, pretrained=False, **kwargs):\n",
    "        super().__init__(\n",
    "            block=models.resnet.BasicBlock,\n",
    "            layers=[2, 2, 2, 2],\n",
    "            num_classes=num_classes,\n",
    "            **kwargs\n",
    "        )\n",
    "        if pretrained:\n",
    "            state_dict = hub.load_state_dict_from_url(\n",
    "                models.resnet.model_urls[\"resnet18\"], progress=True\n",
    "            )\n",
    "            self.load_state_dict(state_dict)\n",
    "\n",
    "        # changes the original resnet architecture to not use fc layers\n",
    "        self.avgpool = nn.AvgPool2d((7, 7))\n",
    "\n",
    "        self.last_conv = nn.Conv2d(\n",
    "            in_channels=self.fc.in_features, out_channels=num_classes, kernel_size=1\n",
    "        )\n",
    "\n",
    "        # copying weights and bias from the fc layer\n",
    "        self.last_conv.weight.data.copy_(\n",
    "            self.fc.weight.data.view(*self.fc.weight.data.shape, 1, 1)\n",
    "        )\n",
    "        self.last_conv.bias.data.copy_(self.fc.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        # Uses conv layer instead of FC, which is used in the original ResNet\n",
    "        x = self.last_conv(x)\n",
    "        return x\n",
    "\n",
    "model = CustomResNet(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model labels\n",
    "The ResNet network is trained on a data set called imagenet. This is a data set with more than 14 million images. All this images are labeled into 1000 clases. The labels can be found in [*imagenet_classes.txt*](https://raw.githubusercontent.com/spmallick/learnopencv/master/PyTorch-Fully-Convolutional-Image-Classification/imagenet_classes.txt).\n",
    "\n",
    "The imagenet file is opened in the cell below. **How do we put the values in the variable called labels?**\n",
    "\n",
    "<details><summary>Hint 1</summary>\n",
    "<p>\n",
    "\n",
    "Search string: *read text file python to list per line*\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2</summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "[line.strip() for line in f.readlines()]\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/imagenet_classes.txt\") as f:\n",
    "    labels = ...\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image\n",
    "This pretrained model can take any image as input. Add more images to the *images* folder, and add the filename in the list below to classify its content. Image sources: [hare](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.ytimg.com%2Fvi%2FrWTV4axEgYw%2Fmaxresdefault.jpg&f=1&nofb=1), [dog](https://res.cloudinary.com/dktx1oojk/image/upload/f_auto,q_90,w_1000,c_scale/web/globalassets/group/breed-tool/images-dogs/siberian_husky.jpg), [gorilla](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fs28164.pcdn.co%2Ffiles%2FWestern-Lowland-Gorilla-0088-8441-1280x720.jpg&f=1&nofb=1)\n",
    "\n",
    "**Create the relative path for an image.**\n",
    "<details><summary>Hint 1</summary>\n",
    "<p>\n",
    "\n",
    "The images are saved in a folder called *images*\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2</summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "f\"images/{images[2]}\"\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\"hare.jpeg\", \"secret_doggy.jpeg\", \"gorilla.jpeg\"]\n",
    "IMAGE_PATH = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = cv2.imread(IMAGE_PATH)\n",
    "print(original_image.shape)\n",
    "rgb_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "plt.axis('off')\n",
    "plt.imshow(rgb_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform image\n",
    "The image is converted to a tensor. The tensor have three channels(red, green and blue), where each of them hold the brightness of their color. This value is in range of 0 to 1. Further the value is normalized based on the mean of all the images in the imagenet data set. Unsqueeze is used to turn the input into expected dimension for the model.\n",
    "\n",
    "**Find the mean and standard deviation for the imagenet data set.**\n",
    "<details><summary>Hint 1</summary>\n",
    "<p>\n",
    "\n",
    "Search string: *mean std imagenet*\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2</summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[...], std=[...]),\n",
    "    ]\n",
    ")\n",
    "    \n",
    "image = transform(rgb_image)\n",
    "print(image.shape)\n",
    "image = image.unsqueeze(0)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting set_grad_enable to false to perform inference without Gradient Calculation. Since we're not going to train our model we do not need to update any weight or biases. As well we do not want to leak test data into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict image class\n",
    "Send our image through the network. Look at the output shape.\n",
    "\n",
    "Why is one of the dimensions 1000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates = model(image)\n",
    "predicates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the classes with highest activation in each of the output cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates = torch.softmax(predicates, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the k best predicated classes.**\n",
    "<details><summary>Hint 1</summary>\n",
    "<p>\n",
    "    \n",
    "[Link](https://stackoverflow.com/a/55802422/11079413)\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2</summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "predicates.topk(NUMBER_OF_TOP_PREDICTIONS, dim=1)\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_TOP_PREDICTIONS = 3\n",
    "preds, class_idexes = ...\n",
    "preds, class_idexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output cell with the highest probability holds the class the network is most certain of.\n",
    "\n",
    "**Fix the prediction print.**\n",
    "<details><summary>Hint 1</summary>\n",
    "<p>\n",
    "    \n",
    "What did you store in the variable *labels*? Which of the variables in the for loop have the certainty?\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2</summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "print(f\"Predicted Class: {labels[predicted_class]} with {round(float(col_max[0]),3)*100}% certainty\")\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUMBER_OF_TOP_PREDICTIONS-1,-1,-1):\n",
    "    pred = preds[:,i]\n",
    "    class_index = class_idexes[:,i]\n",
    "    row_max, row_idx = torch.max(pred, dim=1)\n",
    "    col_max, col_idx = torch.max(row_max, dim=1)\n",
    "    predicted_class = class_index[0, row_idx[0, col_idx], col_idx]\n",
    "    print(f\"Predicted Class: {0} with {0}% certainty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the receptive field\n",
    "The score map shows the probability of the predicted class in each of the output cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_map = predicates[0, predicted_class, :, :].numpy()\n",
    "score_map = score_map[0]\n",
    "print(score_map.shape)\n",
    "score_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the output cells to the original image cells makes it possible to find where the prediction actually was.\n",
    "\n",
    "**Give the score map same size as the original image.**\n",
    "<details><summary>Hint 1</summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "cv2.resize(target, (width, height))   \n",
    "```\n",
    "\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2</summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "score_map = cv2.resize(score_map, (original_image.shape[1], original_image.shape[0]))  \n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_map = ...\n",
    "print(original_image.shape)\n",
    "print(score_map.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarize the score map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, score_map_for_contours = cv2.threshold(\n",
    "    score_map, 0.25, 1, type=cv2.THRESH_BINARY\n",
    ")\n",
    "score_map_for_contours = score_map_for_contours.astype(np.uint8).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a bounding box\n",
    "Find the contour, representation or bounding the shape or form of something, from the binary score map. This contour is used to create a bounding box around the object.\n",
    "\n",
    "**Use the contours to create one bounding box.**\n",
    "<details><summary>Hint 1</summary>\n",
    "<p>\n",
    "\n",
    "[Example](https://www.programcreek.com/python/example/89437/cv2.boundingRect).\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2</summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "cv2.boundingRect(contours[0])\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours, _ = cv2.findContours(\n",
    "    score_map_for_contours,\n",
    "    mode=cv2.RETR_EXTERNAL,\n",
    "    method=cv2.CHAIN_APPROX_SIMPLE,\n",
    ")\n",
    "rect = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the score map to create a mask on the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_map = score_map - np.min(score_map[:])\n",
    "score_map = score_map / np.max(score_map[:])\n",
    "score_map = cv2.cvtColor(score_map, cv2.COLOR_GRAY2BGR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining the score map with the original image and applying the bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_image = (rgb_image * score_map).astype(np.uint8)\n",
    "cv2.rectangle(\n",
    "    masked_image,\n",
    "    rect[:2],\n",
    "    (rect[0] + rect[2], rect[1] + rect[3]),\n",
    "    (255, 0, 0),\n",
    "    2,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the generated images with the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(rgb_image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(score_map);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(masked_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further work\n",
    "\n",
    "- Check that this model works with a random image.\n",
    "- Make the model predict more than one object in an image.\n",
    "- Experiment with another pre trained model.\n",
    "- Create a better receptive field, check out this [blog post](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_cv_course2",
   "language": "python",
   "name": "brain_cv_course2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
